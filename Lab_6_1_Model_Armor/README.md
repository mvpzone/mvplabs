# Lab Title: Prompt Injection and Guardrails with Model Armor

**Author:** Devon Beanish & Anil Kumar Sirikande
**Estimated Time:** 45 minutes

---

## 1. Overview

This lab provides a hands-on introduction to the Model Armor service, focusing on the creation and configuration of security templates. You will begin by defining a new template and then configuring its various safety filters to establish a baseline for model protection.

Once the template is configured, you will evaluate its effectiveness by submitting a series of test prompts designed to simulate common threats. These tests will include examples of Prompt Injection, Jailbreaking, and the submission of Sensitive Data (PII). By analyzing the detailed responses from the service, you will observe how Model Armor detects and mitigates these risks in real-time.


### Learning Objectives

Upon completion of this lab, you will be able to:

* Objective 1:  Understand the role of Model Armor Service and its use of Security templates for LLM Protection
* Objective 2: Create, Configure and update a security template with various safety and content filters
* Objective 3: Test the template's effectiveness against common attacks, including Prompt Injection, Jailbreaking, and Malicious URIs
* Objective 4:  Analyze and interpret the service's response to confirm the successful detection of security threats



## 2. Prerequisites

* Prerequisite 1: Basic knowledge of Python & familiarity with Google Colab Notebooks, such as knowing how to execute code cells
* Prerequisite 2: Basic understanding of LLM and the various threats to LLM like Prompt Injection and Jailbreak
* Prerequisite 3: A Threatspace Google cloud Account, Project id, location to be used
* Prerequisite 4: An Updated version of Chrome/firefox required to access the Google Colab environment


## 3. Lab Instructions

All instructions for this lab are contained within the `Lab 6.1 - Threatspace- Prompt Injection and Guardrails - Model Armor -Lab.ipynb` Jupyter Notebook.

1. Open an Incognito browser window
2. Go to https://coloab.research.google.com
3. Open Notebook -> github -> mvpzone/mvplabs/Lab_6_1_Model_Armor/Lab_6_1_Threatspace_Prompt_Injection_and_Guardails_Model_Armor_Lab.ipynb
4. Open the `Lab 6.1 - Threatspace- Prompt Injection and Guardails - Model Armor -Lab.ipynb` file in a incognito browser 
5. Follow the step-by-step instructions provided in the notebook.
